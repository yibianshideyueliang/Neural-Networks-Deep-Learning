{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    \n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        ####################\n",
    "        '''声明模型对应的参数'''\n",
    "        ####################\n",
    "        self.W1 = tf.Variable(tf.random.normal([28*28, 128], mean=0.0, stddev=0.1), dtype=tf.float32)\n",
    "        self.b1 = tf.Variable(tf.zeros([128]), dtype=tf.float32)\n",
    "        self.W2 = tf.Variable(tf.random.normal([128, 10], mean=0.0, stddev=0.1), dtype=tf.float32)\n",
    "        self.b2 = tf.Variable(tf.zeros([10]), dtype=tf.float32)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        ####################\n",
    "        '''实现模型函数体，返回未归一化的logits'''\n",
    "        ####################\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        h1 = tf.nn.relu(tf.matmul(x, self.W1) + self.b1)\n",
    "        logits = tf.matmul(h1, self.W2) + self.b2\n",
    "        return logits\n",
    "        \n",
    "model = myModel()\n",
    "\n",
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels))\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01*g)\n",
    "\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "\n",
    "    # loss and accuracy is scalar tensor\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def test(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 2.5042467 ; accuracy 0.070766665\n",
      "epoch 1 : loss 2.484389 ; accuracy 0.07271667\n",
      "epoch 2 : loss 2.465681 ; accuracy 0.07416666\n",
      "epoch 3 : loss 2.4479885 ; accuracy 0.07631667\n",
      "epoch 4 : loss 2.4311957 ; accuracy 0.07915\n",
      "epoch 5 : loss 2.4151995 ; accuracy 0.081933334\n",
      "epoch 6 : loss 2.3999124 ; accuracy 0.08471667\n",
      "epoch 7 : loss 2.3852594 ; accuracy 0.087866664\n",
      "epoch 8 : loss 2.3711727 ; accuracy 0.09115\n",
      "epoch 9 : loss 2.3575976 ; accuracy 0.09495\n",
      "epoch 10 : loss 2.3444836 ; accuracy 0.098566666\n",
      "epoch 11 : loss 2.3317885 ; accuracy 0.10243333\n",
      "epoch 12 : loss 2.3194723 ; accuracy 0.107433334\n",
      "epoch 13 : loss 2.3075018 ; accuracy 0.11186667\n",
      "epoch 14 : loss 2.2958455 ; accuracy 0.11688333\n",
      "epoch 15 : loss 2.284478 ; accuracy 0.12213334\n",
      "epoch 16 : loss 2.2733734 ; accuracy 0.1281\n",
      "epoch 17 : loss 2.2625124 ; accuracy 0.13471666\n",
      "epoch 18 : loss 2.2518756 ; accuracy 0.14086667\n",
      "epoch 19 : loss 2.2414463 ; accuracy 0.14736667\n",
      "epoch 20 : loss 2.2312093 ; accuracy 0.1536\n",
      "epoch 21 : loss 2.2211506 ; accuracy 0.16065\n",
      "epoch 22 : loss 2.2112567 ; accuracy 0.16761667\n",
      "epoch 23 : loss 2.2015178 ; accuracy 0.17446667\n",
      "epoch 24 : loss 2.191923 ; accuracy 0.18143333\n",
      "epoch 25 : loss 2.1824632 ; accuracy 0.1891\n",
      "epoch 26 : loss 2.1731322 ; accuracy 0.19641666\n",
      "epoch 27 : loss 2.163921 ; accuracy 0.20445\n",
      "epoch 28 : loss 2.1548228 ; accuracy 0.21166667\n",
      "epoch 29 : loss 2.145831 ; accuracy 0.21921666\n",
      "epoch 30 : loss 2.1369395 ; accuracy 0.22601667\n",
      "epoch 31 : loss 2.1281433 ; accuracy 0.2339\n",
      "epoch 32 : loss 2.1194377 ; accuracy 0.24103333\n",
      "epoch 33 : loss 2.1108196 ; accuracy 0.24856667\n",
      "epoch 34 : loss 2.102285 ; accuracy 0.25601667\n",
      "epoch 35 : loss 2.093829 ; accuracy 0.26351666\n",
      "epoch 36 : loss 2.085449 ; accuracy 0.27138335\n",
      "epoch 37 : loss 2.0771427 ; accuracy 0.27863333\n",
      "epoch 38 : loss 2.0689068 ; accuracy 0.28616667\n",
      "epoch 39 : loss 2.0607386 ; accuracy 0.29315\n",
      "epoch 40 : loss 2.0526347 ; accuracy 0.30048335\n",
      "epoch 41 : loss 2.044594 ; accuracy 0.30726665\n",
      "epoch 42 : loss 2.0366147 ; accuracy 0.31448334\n",
      "epoch 43 : loss 2.0286937 ; accuracy 0.32153332\n",
      "epoch 44 : loss 2.0208282 ; accuracy 0.3286\n",
      "epoch 45 : loss 2.0130172 ; accuracy 0.33475\n",
      "epoch 46 : loss 2.0052593 ; accuracy 0.34146667\n",
      "epoch 47 : loss 1.9975538 ; accuracy 0.34756666\n",
      "epoch 48 : loss 1.9898993 ; accuracy 0.35428333\n",
      "epoch 49 : loss 1.9822946 ; accuracy 0.35956666\n",
      "epoch 50 : loss 1.9747378 ; accuracy 0.36571667\n",
      "epoch 51 : loss 1.9672279 ; accuracy 0.37185\n",
      "epoch 52 : loss 1.9597638 ; accuracy 0.37698334\n",
      "epoch 53 : loss 1.9523449 ; accuracy 0.3823\n",
      "epoch 54 : loss 1.9449718 ; accuracy 0.388\n",
      "epoch 55 : loss 1.9376423 ; accuracy 0.39383334\n",
      "epoch 56 : loss 1.930356 ; accuracy 0.3989\n",
      "epoch 57 : loss 1.9231117 ; accuracy 0.40385\n",
      "epoch 58 : loss 1.9159089 ; accuracy 0.40876666\n",
      "epoch 59 : loss 1.9087466 ; accuracy 0.4137\n",
      "epoch 60 : loss 1.9016248 ; accuracy 0.41881666\n",
      "epoch 61 : loss 1.8945429 ; accuracy 0.42401665\n",
      "epoch 62 : loss 1.8875011 ; accuracy 0.42911667\n",
      "epoch 63 : loss 1.8804978 ; accuracy 0.43391666\n",
      "epoch 64 : loss 1.8735328 ; accuracy 0.43825\n",
      "epoch 65 : loss 1.866605 ; accuracy 0.4423\n",
      "epoch 66 : loss 1.8597139 ; accuracy 0.44625\n",
      "epoch 67 : loss 1.8528596 ; accuracy 0.45113334\n",
      "epoch 68 : loss 1.8460419 ; accuracy 0.4549\n",
      "epoch 69 : loss 1.8392615 ; accuracy 0.45851666\n",
      "epoch 70 : loss 1.8325177 ; accuracy 0.46215\n",
      "epoch 71 : loss 1.8258102 ; accuracy 0.46613333\n",
      "epoch 72 : loss 1.8191372 ; accuracy 0.47026667\n",
      "epoch 73 : loss 1.8124992 ; accuracy 0.47416666\n",
      "epoch 74 : loss 1.8058957 ; accuracy 0.4779\n",
      "epoch 75 : loss 1.7993269 ; accuracy 0.4817\n",
      "epoch 76 : loss 1.7927924 ; accuracy 0.48513332\n",
      "epoch 77 : loss 1.7862915 ; accuracy 0.48901665\n",
      "epoch 78 : loss 1.7798247 ; accuracy 0.4923\n",
      "epoch 79 : loss 1.7733914 ; accuracy 0.49565\n",
      "epoch 80 : loss 1.7669908 ; accuracy 0.49903333\n",
      "epoch 81 : loss 1.7606232 ; accuracy 0.5023\n",
      "epoch 82 : loss 1.7542887 ; accuracy 0.5057\n",
      "epoch 83 : loss 1.747987 ; accuracy 0.50881666\n",
      "epoch 84 : loss 1.7417177 ; accuracy 0.5118667\n",
      "epoch 85 : loss 1.735481 ; accuracy 0.51496667\n",
      "epoch 86 : loss 1.7292761 ; accuracy 0.51823336\n",
      "epoch 87 : loss 1.7231032 ; accuracy 0.52166665\n",
      "epoch 88 : loss 1.7169621 ; accuracy 0.52451664\n",
      "epoch 89 : loss 1.7108537 ; accuracy 0.52745\n",
      "epoch 90 : loss 1.7047765 ; accuracy 0.53041667\n",
      "epoch 91 : loss 1.6987315 ; accuracy 0.53346664\n",
      "epoch 92 : loss 1.6927176 ; accuracy 0.53648335\n",
      "epoch 93 : loss 1.6867352 ; accuracy 0.5394833\n",
      "epoch 94 : loss 1.6807834 ; accuracy 0.5417333\n",
      "epoch 95 : loss 1.6748624 ; accuracy 0.54408336\n",
      "epoch 96 : loss 1.6689724 ; accuracy 0.54635\n",
      "epoch 97 : loss 1.6631134 ; accuracy 0.54915\n",
      "epoch 98 : loss 1.6572856 ; accuracy 0.55163336\n",
      "epoch 99 : loss 1.651488 ; accuracy 0.55413336\n",
      "epoch 100 : loss 1.6457206 ; accuracy 0.55628335\n",
      "epoch 101 : loss 1.6399838 ; accuracy 0.5585667\n",
      "epoch 102 : loss 1.6342778 ; accuracy 0.5610167\n",
      "epoch 103 : loss 1.6286025 ; accuracy 0.5632333\n",
      "epoch 104 : loss 1.622957 ; accuracy 0.5654333\n",
      "epoch 105 : loss 1.6173409 ; accuracy 0.5675333\n",
      "epoch 106 : loss 1.6117544 ; accuracy 0.5697\n",
      "epoch 107 : loss 1.6061974 ; accuracy 0.57163334\n",
      "epoch 108 : loss 1.6006701 ; accuracy 0.57381666\n",
      "epoch 109 : loss 1.5951722 ; accuracy 0.576\n",
      "epoch 110 : loss 1.5897031 ; accuracy 0.57818335\n",
      "epoch 111 : loss 1.584263 ; accuracy 0.5802\n",
      "epoch 112 : loss 1.5788518 ; accuracy 0.5821667\n",
      "epoch 113 : loss 1.5734693 ; accuracy 0.5840833\n",
      "epoch 114 : loss 1.5681158 ; accuracy 0.5857\n",
      "epoch 115 : loss 1.562791 ; accuracy 0.5876\n",
      "epoch 116 : loss 1.5574943 ; accuracy 0.58926666\n",
      "epoch 117 : loss 1.5522255 ; accuracy 0.5912667\n",
      "epoch 118 : loss 1.5469853 ; accuracy 0.593\n",
      "epoch 119 : loss 1.541773 ; accuracy 0.5948167\n",
      "epoch 120 : loss 1.5365885 ; accuracy 0.59676665\n",
      "epoch 121 : loss 1.5314323 ; accuracy 0.59853333\n",
      "epoch 122 : loss 1.5263045 ; accuracy 0.60035\n",
      "epoch 123 : loss 1.5212048 ; accuracy 0.60248333\n",
      "epoch 124 : loss 1.5161326 ; accuracy 0.6046\n",
      "epoch 125 : loss 1.5110879 ; accuracy 0.6062667\n",
      "epoch 126 : loss 1.5060709 ; accuracy 0.6078333\n",
      "epoch 127 : loss 1.5010809 ; accuracy 0.60948336\n",
      "epoch 128 : loss 1.4961187 ; accuracy 0.6109167\n",
      "epoch 129 : loss 1.4911841 ; accuracy 0.61255\n",
      "epoch 130 : loss 1.4862764 ; accuracy 0.6142333\n",
      "epoch 131 : loss 1.4813958 ; accuracy 0.6156333\n",
      "epoch 132 : loss 1.4765425 ; accuracy 0.61721665\n",
      "epoch 133 : loss 1.471716 ; accuracy 0.61865\n",
      "epoch 134 : loss 1.4669168 ; accuracy 0.62053335\n",
      "epoch 135 : loss 1.4621445 ; accuracy 0.62245\n",
      "epoch 136 : loss 1.457399 ; accuracy 0.6239667\n",
      "epoch 137 : loss 1.4526796 ; accuracy 0.6252667\n",
      "epoch 138 : loss 1.4479868 ; accuracy 0.62668335\n",
      "epoch 139 : loss 1.4433209 ; accuracy 0.62825\n",
      "epoch 140 : loss 1.438681 ; accuracy 0.6296\n",
      "epoch 141 : loss 1.4340671 ; accuracy 0.63091666\n",
      "epoch 142 : loss 1.4294795 ; accuracy 0.63241667\n",
      "epoch 143 : loss 1.4249178 ; accuracy 0.6339833\n",
      "epoch 144 : loss 1.4203818 ; accuracy 0.63531667\n",
      "epoch 145 : loss 1.4158716 ; accuracy 0.6364833\n",
      "epoch 146 : loss 1.411387 ; accuracy 0.63785\n",
      "epoch 147 : loss 1.4069282 ; accuracy 0.63916665\n",
      "epoch 148 : loss 1.4024945 ; accuracy 0.64038336\n",
      "epoch 149 : loss 1.3980862 ; accuracy 0.64185\n",
      "epoch 150 : loss 1.3937029 ; accuracy 0.643\n",
      "epoch 151 : loss 1.3893442 ; accuracy 0.6446667\n",
      "epoch 152 : loss 1.3850106 ; accuracy 0.64598334\n",
      "epoch 153 : loss 1.3807012 ; accuracy 0.6472833\n",
      "epoch 154 : loss 1.3764169 ; accuracy 0.64841664\n",
      "epoch 155 : loss 1.3721573 ; accuracy 0.64956665\n",
      "epoch 156 : loss 1.3679218 ; accuracy 0.6508167\n",
      "epoch 157 : loss 1.3637102 ; accuracy 0.65201664\n",
      "epoch 158 : loss 1.3595221 ; accuracy 0.6531\n",
      "epoch 159 : loss 1.3553576 ; accuracy 0.65423334\n",
      "epoch 160 : loss 1.3512168 ; accuracy 0.65561664\n",
      "epoch 161 : loss 1.3470994 ; accuracy 0.6566\n",
      "epoch 162 : loss 1.3430057 ; accuracy 0.6577\n",
      "epoch 163 : loss 1.3389349 ; accuracy 0.65893334\n",
      "epoch 164 : loss 1.3348875 ; accuracy 0.6601\n",
      "epoch 165 : loss 1.3308632 ; accuracy 0.6612333\n",
      "epoch 166 : loss 1.3268617 ; accuracy 0.66223335\n",
      "epoch 167 : loss 1.3228831 ; accuracy 0.6636\n",
      "epoch 168 : loss 1.3189266 ; accuracy 0.6648667\n",
      "epoch 169 : loss 1.3149928 ; accuracy 0.6663\n",
      "epoch 170 : loss 1.3110814 ; accuracy 0.66753334\n",
      "epoch 171 : loss 1.3071927 ; accuracy 0.66858333\n",
      "epoch 172 : loss 1.3033266 ; accuracy 0.6696333\n",
      "epoch 173 : loss 1.2994826 ; accuracy 0.67083335\n",
      "epoch 174 : loss 1.2956607 ; accuracy 0.67193335\n",
      "epoch 175 : loss 1.2918607 ; accuracy 0.67296666\n",
      "epoch 176 : loss 1.2880821 ; accuracy 0.6741\n",
      "epoch 177 : loss 1.2843252 ; accuracy 0.6752833\n",
      "epoch 178 : loss 1.2805898 ; accuracy 0.6763833\n",
      "epoch 179 : loss 1.2768763 ; accuracy 0.67758334\n",
      "epoch 180 : loss 1.2731841 ; accuracy 0.67855\n",
      "epoch 181 : loss 1.2695132 ; accuracy 0.6796167\n",
      "epoch 182 : loss 1.2658635 ; accuracy 0.68053335\n",
      "epoch 183 : loss 1.2622349 ; accuracy 0.68163335\n",
      "epoch 184 : loss 1.2586273 ; accuracy 0.6824333\n",
      "epoch 185 : loss 1.2550408 ; accuracy 0.6832167\n",
      "epoch 186 : loss 1.2514747 ; accuracy 0.68438333\n",
      "epoch 187 : loss 1.2479297 ; accuracy 0.6856167\n",
      "epoch 188 : loss 1.2444049 ; accuracy 0.6865\n",
      "epoch 189 : loss 1.2409003 ; accuracy 0.6875833\n",
      "epoch 190 : loss 1.2374161 ; accuracy 0.68841666\n",
      "epoch 191 : loss 1.233952 ; accuracy 0.6893333\n",
      "epoch 192 : loss 1.230508 ; accuracy 0.69025\n",
      "epoch 193 : loss 1.2270838 ; accuracy 0.6911\n",
      "epoch 194 : loss 1.2236799 ; accuracy 0.69203335\n",
      "epoch 195 : loss 1.2202955 ; accuracy 0.6928667\n",
      "epoch 196 : loss 1.2169311 ; accuracy 0.69378334\n",
      "epoch 197 : loss 1.2135857 ; accuracy 0.6950333\n",
      "epoch 198 : loss 1.2102593 ; accuracy 0.69586664\n",
      "epoch 199 : loss 1.2069525 ; accuracy 0.6968333\n",
      "epoch 200 : loss 1.2036644 ; accuracy 0.69781667\n",
      "epoch 201 : loss 1.2003946 ; accuracy 0.69878334\n",
      "epoch 202 : loss 1.1971439 ; accuracy 0.6996667\n",
      "epoch 203 : loss 1.193912 ; accuracy 0.7003667\n",
      "epoch 204 : loss 1.1906987 ; accuracy 0.7013\n",
      "epoch 205 : loss 1.1875038 ; accuracy 0.70196664\n",
      "epoch 206 : loss 1.1843272 ; accuracy 0.7029167\n",
      "epoch 207 : loss 1.1811689 ; accuracy 0.7037\n",
      "epoch 208 : loss 1.1780283 ; accuracy 0.70458335\n",
      "epoch 209 : loss 1.1749063 ; accuracy 0.7054167\n",
      "epoch 210 : loss 1.1718019 ; accuracy 0.70608336\n",
      "epoch 211 : loss 1.1687155 ; accuracy 0.7068667\n",
      "epoch 212 : loss 1.1656467 ; accuracy 0.70778334\n",
      "epoch 213 : loss 1.1625953 ; accuracy 0.7086167\n",
      "epoch 214 : loss 1.1595615 ; accuracy 0.70918334\n",
      "epoch 215 : loss 1.1565448 ; accuracy 0.71\n",
      "epoch 216 : loss 1.1535453 ; accuracy 0.71066666\n",
      "epoch 217 : loss 1.1505628 ; accuracy 0.7116167\n",
      "epoch 218 : loss 1.147597 ; accuracy 0.71246666\n",
      "epoch 219 : loss 1.1446481 ; accuracy 0.7133667\n",
      "epoch 220 : loss 1.1417158 ; accuracy 0.7143833\n",
      "epoch 221 : loss 1.1388003 ; accuracy 0.7150667\n",
      "epoch 222 : loss 1.1359011 ; accuracy 0.7158167\n",
      "epoch 223 : loss 1.1330185 ; accuracy 0.71636665\n",
      "epoch 224 : loss 1.1301522 ; accuracy 0.7170333\n",
      "epoch 225 : loss 1.1273019 ; accuracy 0.71781665\n",
      "epoch 226 : loss 1.1244675 ; accuracy 0.7184333\n",
      "epoch 227 : loss 1.1216491 ; accuracy 0.719\n",
      "epoch 228 : loss 1.1188463 ; accuracy 0.71968335\n",
      "epoch 229 : loss 1.1160591 ; accuracy 0.72065\n",
      "epoch 230 : loss 1.1132878 ; accuracy 0.7215667\n",
      "epoch 231 : loss 1.1105319 ; accuracy 0.72213334\n",
      "epoch 232 : loss 1.1077914 ; accuracy 0.72315\n",
      "epoch 233 : loss 1.1050664 ; accuracy 0.7237333\n",
      "epoch 234 : loss 1.1023566 ; accuracy 0.7244\n",
      "epoch 235 : loss 1.0996617 ; accuracy 0.72498333\n",
      "epoch 236 : loss 1.0969819 ; accuracy 0.72576666\n",
      "epoch 237 : loss 1.094317 ; accuracy 0.7265667\n",
      "epoch 238 : loss 1.0916667 ; accuracy 0.7270833\n",
      "epoch 239 : loss 1.0890316 ; accuracy 0.7278\n",
      "epoch 240 : loss 1.0864114 ; accuracy 0.7284333\n",
      "epoch 241 : loss 1.0838059 ; accuracy 0.7291833\n",
      "epoch 242 : loss 1.081215 ; accuracy 0.72968334\n",
      "epoch 243 : loss 1.0786387 ; accuracy 0.7302833\n",
      "epoch 244 : loss 1.0760764 ; accuracy 0.73088336\n",
      "epoch 245 : loss 1.0735283 ; accuracy 0.73165\n",
      "epoch 246 : loss 1.0709944 ; accuracy 0.73228335\n",
      "epoch 247 : loss 1.0684743 ; accuracy 0.73291665\n",
      "epoch 248 : loss 1.0659683 ; accuracy 0.7336\n",
      "epoch 249 : loss 1.0634758 ; accuracy 0.73426664\n",
      "epoch 250 : loss 1.0609969 ; accuracy 0.73495\n",
      "epoch 251 : loss 1.0585314 ; accuracy 0.73545\n",
      "epoch 252 : loss 1.0560793 ; accuracy 0.7360333\n",
      "epoch 253 : loss 1.0536406 ; accuracy 0.7367167\n",
      "epoch 254 : loss 1.0512154 ; accuracy 0.73731667\n",
      "epoch 255 : loss 1.0488033 ; accuracy 0.7378833\n",
      "epoch 256 : loss 1.0464045 ; accuracy 0.7387\n",
      "epoch 257 : loss 1.0440189 ; accuracy 0.7394\n",
      "epoch 258 : loss 1.0416464 ; accuracy 0.73995\n",
      "epoch 259 : loss 1.039287 ; accuracy 0.7406167\n",
      "epoch 260 : loss 1.0369403 ; accuracy 0.74111664\n",
      "epoch 261 : loss 1.0346067 ; accuracy 0.7417833\n",
      "epoch 262 : loss 1.0322855 ; accuracy 0.7423667\n",
      "epoch 263 : loss 1.0299768 ; accuracy 0.74306667\n",
      "epoch 264 : loss 1.0276806 ; accuracy 0.74365\n",
      "epoch 265 : loss 1.0253966 ; accuracy 0.7443\n",
      "epoch 266 : loss 1.023125 ; accuracy 0.74478334\n",
      "epoch 267 : loss 1.0208654 ; accuracy 0.74545\n",
      "epoch 268 : loss 1.0186181 ; accuracy 0.74593335\n",
      "epoch 269 : loss 1.0163829 ; accuracy 0.74655\n",
      "epoch 270 : loss 1.0141597 ; accuracy 0.7472\n",
      "epoch 271 : loss 1.0119483 ; accuracy 0.74773335\n",
      "epoch 272 : loss 1.0097489 ; accuracy 0.7484\n",
      "epoch 273 : loss 1.0075614 ; accuracy 0.74885\n",
      "epoch 274 : loss 1.0053855 ; accuracy 0.74941665\n",
      "epoch 275 : loss 1.0032215 ; accuracy 0.7500333\n",
      "epoch 276 : loss 1.0010691 ; accuracy 0.75075\n",
      "epoch 277 : loss 0.998928 ; accuracy 0.75123334\n",
      "epoch 278 : loss 0.99679834 ; accuracy 0.75168335\n",
      "epoch 279 : loss 0.9946798 ; accuracy 0.75221664\n",
      "epoch 280 : loss 0.99257267 ; accuracy 0.7527\n",
      "epoch 281 : loss 0.9904766 ; accuracy 0.7532667\n",
      "epoch 282 : loss 0.98839176 ; accuracy 0.75385\n",
      "epoch 283 : loss 0.9863177 ; accuracy 0.7542167\n",
      "epoch 284 : loss 0.9842544 ; accuracy 0.7546333\n",
      "epoch 285 : loss 0.9822023 ; accuracy 0.75505\n",
      "epoch 286 : loss 0.98016095 ; accuracy 0.7556667\n",
      "epoch 287 : loss 0.97813034 ; accuracy 0.75605\n",
      "epoch 288 : loss 0.97611016 ; accuracy 0.7565\n",
      "epoch 289 : loss 0.97410053 ; accuracy 0.75698334\n",
      "epoch 290 : loss 0.97210115 ; accuracy 0.75736666\n",
      "epoch 291 : loss 0.97011197 ; accuracy 0.7579333\n",
      "epoch 292 : loss 0.96813315 ; accuracy 0.75845\n",
      "epoch 293 : loss 0.9661646 ; accuracy 0.7586833\n",
      "epoch 294 : loss 0.9642064 ; accuracy 0.75916666\n",
      "epoch 295 : loss 0.9622583 ; accuracy 0.75993335\n",
      "epoch 296 : loss 0.9603203 ; accuracy 0.7604333\n",
      "epoch 297 : loss 0.95839226 ; accuracy 0.76088333\n",
      "epoch 298 : loss 0.9564741 ; accuracy 0.7612333\n",
      "epoch 299 : loss 0.95456576 ; accuracy 0.76163334\n",
      "epoch 300 : loss 0.9526671 ; accuracy 0.76213336\n",
      "epoch 301 : loss 0.9507781 ; accuracy 0.7625333\n",
      "epoch 302 : loss 0.94889855 ; accuracy 0.76303333\n",
      "epoch 303 : loss 0.94702864 ; accuracy 0.76365\n",
      "epoch 304 : loss 0.94516814 ; accuracy 0.76413333\n",
      "epoch 305 : loss 0.9433172 ; accuracy 0.7645\n",
      "epoch 306 : loss 0.9414754 ; accuracy 0.7649\n",
      "epoch 307 : loss 0.9396429 ; accuracy 0.7653\n",
      "epoch 308 : loss 0.93781966 ; accuracy 0.7658\n",
      "epoch 309 : loss 0.9360056 ; accuracy 0.76635\n",
      "epoch 310 : loss 0.93420076 ; accuracy 0.76698333\n",
      "epoch 311 : loss 0.9324049 ; accuracy 0.76743335\n",
      "epoch 312 : loss 0.93061805 ; accuracy 0.7678\n",
      "epoch 313 : loss 0.9288401 ; accuracy 0.76813334\n",
      "epoch 314 : loss 0.92707115 ; accuracy 0.7687333\n",
      "epoch 315 : loss 0.92531085 ; accuracy 0.76921666\n",
      "epoch 316 : loss 0.92355937 ; accuracy 0.7697667\n",
      "epoch 317 : loss 0.92181665 ; accuracy 0.77025\n",
      "epoch 318 : loss 0.92008257 ; accuracy 0.7708\n",
      "epoch 319 : loss 0.9183569 ; accuracy 0.7712167\n",
      "epoch 320 : loss 0.9166399 ; accuracy 0.77161664\n",
      "epoch 321 : loss 0.91493136 ; accuracy 0.7718\n",
      "epoch 322 : loss 0.9132311 ; accuracy 0.77215\n",
      "epoch 323 : loss 0.9115391 ; accuracy 0.77255\n",
      "epoch 324 : loss 0.90985537 ; accuracy 0.7729333\n",
      "epoch 325 : loss 0.90817976 ; accuracy 0.7734333\n",
      "epoch 326 : loss 0.9065124 ; accuracy 0.7738\n",
      "epoch 327 : loss 0.90485334 ; accuracy 0.77426666\n",
      "epoch 328 : loss 0.9032024 ; accuracy 0.7747667\n",
      "epoch 329 : loss 0.9015594 ; accuracy 0.77525\n",
      "epoch 330 : loss 0.89992446 ; accuracy 0.7755833\n",
      "epoch 331 : loss 0.89829737 ; accuracy 0.77605\n",
      "epoch 332 : loss 0.896678 ; accuracy 0.7765\n",
      "epoch 333 : loss 0.8950665 ; accuracy 0.7769833\n",
      "epoch 334 : loss 0.89346266 ; accuracy 0.77736664\n",
      "epoch 335 : loss 0.8918666 ; accuracy 0.77775\n",
      "epoch 336 : loss 0.8902781 ; accuracy 0.7779833\n",
      "epoch 337 : loss 0.88869727 ; accuracy 0.77835\n",
      "epoch 338 : loss 0.887124 ; accuracy 0.77891666\n",
      "epoch 339 : loss 0.8855583 ; accuracy 0.77915\n",
      "epoch 340 : loss 0.88400006 ; accuracy 0.7794167\n",
      "epoch 341 : loss 0.88244915 ; accuracy 0.77963334\n",
      "epoch 342 : loss 0.8809056 ; accuracy 0.77985\n",
      "epoch 343 : loss 0.8793693 ; accuracy 0.7801833\n",
      "epoch 344 : loss 0.8778403 ; accuracy 0.7805167\n",
      "epoch 345 : loss 0.87631863 ; accuracy 0.7809\n",
      "epoch 346 : loss 0.87480414 ; accuracy 0.78131664\n",
      "epoch 347 : loss 0.87329686 ; accuracy 0.78178334\n",
      "epoch 348 : loss 0.87179655 ; accuracy 0.78213334\n",
      "epoch 349 : loss 0.8703032 ; accuracy 0.78246665\n",
      "epoch 350 : loss 0.8688168 ; accuracy 0.78286666\n",
      "epoch 351 : loss 0.8673372 ; accuracy 0.78326666\n",
      "epoch 352 : loss 0.8658646 ; accuracy 0.7837167\n",
      "epoch 353 : loss 0.86439896 ; accuracy 0.7841167\n",
      "epoch 354 : loss 0.86294013 ; accuracy 0.78438336\n",
      "epoch 355 : loss 0.86148816 ; accuracy 0.7847833\n",
      "epoch 356 : loss 0.860043 ; accuracy 0.7851\n",
      "epoch 357 : loss 0.8586045 ; accuracy 0.7854667\n",
      "epoch 358 : loss 0.8571727 ; accuracy 0.7859\n",
      "epoch 359 : loss 0.8557475 ; accuracy 0.7861\n",
      "epoch 360 : loss 0.8543289 ; accuracy 0.78651667\n",
      "epoch 361 : loss 0.85291654 ; accuracy 0.78681666\n",
      "epoch 362 : loss 0.8515107 ; accuracy 0.78705\n",
      "epoch 363 : loss 0.850111 ; accuracy 0.7873333\n",
      "epoch 364 : loss 0.8487177 ; accuracy 0.7877333\n",
      "epoch 365 : loss 0.84733087 ; accuracy 0.7880333\n",
      "epoch 366 : loss 0.84595025 ; accuracy 0.7883833\n",
      "epoch 367 : loss 0.84457606 ; accuracy 0.78868335\n",
      "epoch 368 : loss 0.843208 ; accuracy 0.78898335\n",
      "epoch 369 : loss 0.8418461 ; accuracy 0.78921664\n",
      "epoch 370 : loss 0.8404903 ; accuracy 0.7895167\n",
      "epoch 371 : loss 0.83914053 ; accuracy 0.78966665\n",
      "epoch 372 : loss 0.8377969 ; accuracy 0.7899167\n",
      "epoch 373 : loss 0.8364592 ; accuracy 0.7902\n",
      "epoch 374 : loss 0.8351275 ; accuracy 0.7905333\n",
      "epoch 375 : loss 0.8338016 ; accuracy 0.79073334\n",
      "epoch 376 : loss 0.83248174 ; accuracy 0.7909167\n",
      "epoch 377 : loss 0.8311677 ; accuracy 0.79123336\n",
      "epoch 378 : loss 0.8298597 ; accuracy 0.79158336\n",
      "epoch 379 : loss 0.8285575 ; accuracy 0.79188335\n",
      "epoch 380 : loss 0.82726103 ; accuracy 0.79226667\n",
      "epoch 381 : loss 0.8259703 ; accuracy 0.7924333\n",
      "epoch 382 : loss 0.8246853 ; accuracy 0.79261667\n",
      "epoch 383 : loss 0.8234059 ; accuracy 0.79288334\n",
      "epoch 384 : loss 0.8221323 ; accuracy 0.7930833\n",
      "epoch 385 : loss 0.8208643 ; accuracy 0.79345\n",
      "epoch 386 : loss 0.81960195 ; accuracy 0.79373336\n",
      "epoch 387 : loss 0.8183452 ; accuracy 0.7941667\n",
      "epoch 388 : loss 0.8170938 ; accuracy 0.7944\n",
      "epoch 389 : loss 0.8158479 ; accuracy 0.79475\n",
      "epoch 390 : loss 0.81460726 ; accuracy 0.795\n",
      "epoch 391 : loss 0.81337214 ; accuracy 0.7952667\n",
      "epoch 392 : loss 0.8121423 ; accuracy 0.79543334\n",
      "epoch 393 : loss 0.81091785 ; accuracy 0.79571664\n",
      "epoch 394 : loss 0.8096987 ; accuracy 0.7960167\n",
      "epoch 395 : loss 0.80848485 ; accuracy 0.7963167\n",
      "epoch 396 : loss 0.8072762 ; accuracy 0.79655\n",
      "epoch 397 : loss 0.80607265 ; accuracy 0.7969\n",
      "epoch 398 : loss 0.80487436 ; accuracy 0.79725\n",
      "epoch 399 : loss 0.80368125 ; accuracy 0.79753333\n",
      "epoch 400 : loss 0.80249304 ; accuracy 0.7978\n",
      "epoch 401 : loss 0.80130994 ; accuracy 0.79803336\n",
      "epoch 402 : loss 0.80013186 ; accuracy 0.79831666\n",
      "epoch 403 : loss 0.7989587 ; accuracy 0.79861665\n",
      "epoch 404 : loss 0.79779065 ; accuracy 0.7988667\n",
      "epoch 405 : loss 0.79662734 ; accuracy 0.7992667\n",
      "epoch 406 : loss 0.7954689 ; accuracy 0.7995667\n",
      "epoch 407 : loss 0.7943153 ; accuracy 0.79971665\n",
      "epoch 408 : loss 0.7931665 ; accuracy 0.7998667\n",
      "epoch 409 : loss 0.7920225 ; accuracy 0.8002167\n",
      "epoch 410 : loss 0.7908832 ; accuracy 0.80045\n",
      "epoch 411 : loss 0.78974885 ; accuracy 0.80081666\n",
      "epoch 412 : loss 0.7886193 ; accuracy 0.801\n",
      "epoch 413 : loss 0.78749424 ; accuracy 0.8013667\n",
      "epoch 414 : loss 0.78637403 ; accuracy 0.80163336\n",
      "epoch 415 : loss 0.78525853 ; accuracy 0.8018\n",
      "epoch 416 : loss 0.7841476 ; accuracy 0.80196667\n",
      "epoch 417 : loss 0.7830413 ; accuracy 0.80216664\n",
      "epoch 418 : loss 0.7819395 ; accuracy 0.80233335\n",
      "epoch 419 : loss 0.78084224 ; accuracy 0.8026\n",
      "epoch 420 : loss 0.77974945 ; accuracy 0.80296665\n",
      "epoch 421 : loss 0.7786612 ; accuracy 0.8032333\n",
      "epoch 422 : loss 0.7775774 ; accuracy 0.80343336\n",
      "epoch 423 : loss 0.77649796 ; accuracy 0.80378336\n",
      "epoch 424 : loss 0.77542305 ; accuracy 0.8039333\n",
      "epoch 425 : loss 0.7743525 ; accuracy 0.80408335\n",
      "epoch 426 : loss 0.7732862 ; accuracy 0.80431664\n",
      "epoch 427 : loss 0.7722243 ; accuracy 0.8046\n",
      "epoch 428 : loss 0.7711667 ; accuracy 0.8048\n",
      "epoch 429 : loss 0.7701134 ; accuracy 0.80511665\n",
      "epoch 430 : loss 0.76906437 ; accuracy 0.8052833\n",
      "epoch 431 : loss 0.76801956 ; accuracy 0.8056333\n",
      "epoch 432 : loss 0.7669788 ; accuracy 0.80593336\n",
      "epoch 433 : loss 0.7659422 ; accuracy 0.80611664\n",
      "epoch 434 : loss 0.7649099 ; accuracy 0.8064833\n",
      "epoch 435 : loss 0.76388186 ; accuracy 0.80675\n",
      "epoch 436 : loss 0.7628578 ; accuracy 0.8070167\n",
      "epoch 437 : loss 0.761838 ; accuracy 0.80726665\n",
      "epoch 438 : loss 0.76082224 ; accuracy 0.8074667\n",
      "epoch 439 : loss 0.75981057 ; accuracy 0.8077833\n",
      "epoch 440 : loss 0.7588029 ; accuracy 0.8079\n",
      "epoch 441 : loss 0.757799 ; accuracy 0.80825\n",
      "epoch 442 : loss 0.75679934 ; accuracy 0.80843335\n",
      "epoch 443 : loss 0.7558035 ; accuracy 0.80868334\n",
      "epoch 444 : loss 0.7548117 ; accuracy 0.8088833\n",
      "epoch 445 : loss 0.7538238 ; accuracy 0.8091\n",
      "epoch 446 : loss 0.75283986 ; accuracy 0.80936664\n",
      "epoch 447 : loss 0.7518598 ; accuracy 0.80955\n",
      "epoch 448 : loss 0.75088346 ; accuracy 0.80983335\n",
      "epoch 449 : loss 0.74991095 ; accuracy 0.8101\n",
      "epoch 450 : loss 0.7489422 ; accuracy 0.8103833\n",
      "epoch 451 : loss 0.7479771 ; accuracy 0.8106667\n",
      "epoch 452 : loss 0.7470157 ; accuracy 0.81083333\n",
      "epoch 453 : loss 0.74605817 ; accuracy 0.811\n",
      "epoch 454 : loss 0.7451043 ; accuracy 0.81125\n",
      "epoch 455 : loss 0.7441541 ; accuracy 0.8115\n",
      "epoch 456 : loss 0.7432076 ; accuracy 0.81175\n",
      "epoch 457 : loss 0.74226457 ; accuracy 0.81198335\n",
      "epoch 458 : loss 0.74132514 ; accuracy 0.8123\n",
      "epoch 459 : loss 0.74038935 ; accuracy 0.81261665\n",
      "epoch 460 : loss 0.7394572 ; accuracy 0.8128\n",
      "epoch 461 : loss 0.73852867 ; accuracy 0.81306666\n",
      "epoch 462 : loss 0.7376036 ; accuracy 0.8132833\n",
      "epoch 463 : loss 0.7366822 ; accuracy 0.8135\n",
      "epoch 464 : loss 0.73576427 ; accuracy 0.81375\n",
      "epoch 465 : loss 0.73484975 ; accuracy 0.81385\n",
      "epoch 466 : loss 0.7339387 ; accuracy 0.81408334\n",
      "epoch 467 : loss 0.7330313 ; accuracy 0.81423336\n",
      "epoch 468 : loss 0.73212713 ; accuracy 0.81455\n",
      "epoch 469 : loss 0.7312265 ; accuracy 0.8149\n",
      "epoch 470 : loss 0.7303293 ; accuracy 0.8151\n",
      "epoch 471 : loss 0.72943544 ; accuracy 0.81521666\n",
      "epoch 472 : loss 0.72854495 ; accuracy 0.8154\n",
      "epoch 473 : loss 0.7276577 ; accuracy 0.8157\n",
      "epoch 474 : loss 0.7267738 ; accuracy 0.8158\n",
      "epoch 475 : loss 0.7258932 ; accuracy 0.81595\n",
      "epoch 476 : loss 0.72501594 ; accuracy 0.81615\n",
      "epoch 477 : loss 0.72414184 ; accuracy 0.8163667\n",
      "epoch 478 : loss 0.7232711 ; accuracy 0.81663334\n",
      "epoch 479 : loss 0.7224036 ; accuracy 0.81695\n",
      "epoch 480 : loss 0.7215393 ; accuracy 0.8171667\n",
      "epoch 481 : loss 0.72067827 ; accuracy 0.8173\n",
      "epoch 482 : loss 0.7198203 ; accuracy 0.81738335\n",
      "epoch 483 : loss 0.71896577 ; accuracy 0.8175667\n",
      "epoch 484 : loss 0.7181142 ; accuracy 0.81776667\n",
      "epoch 485 : loss 0.7172659 ; accuracy 0.8179333\n",
      "epoch 486 : loss 0.7164207 ; accuracy 0.81813335\n",
      "epoch 487 : loss 0.7155785 ; accuracy 0.81835\n",
      "epoch 488 : loss 0.71473944 ; accuracy 0.81855\n",
      "epoch 489 : loss 0.71390337 ; accuracy 0.8188\n",
      "epoch 490 : loss 0.7130705 ; accuracy 0.81895\n",
      "epoch 491 : loss 0.7122406 ; accuracy 0.81905\n",
      "epoch 492 : loss 0.71141374 ; accuracy 0.81918335\n",
      "epoch 493 : loss 0.7105898 ; accuracy 0.8193167\n",
      "epoch 494 : loss 0.709769 ; accuracy 0.81941664\n",
      "epoch 495 : loss 0.7089512 ; accuracy 0.81955\n",
      "epoch 496 : loss 0.7081362 ; accuracy 0.8197167\n",
      "epoch 497 : loss 0.7073242 ; accuracy 0.81985\n",
      "epoch 498 : loss 0.70651513 ; accuracy 0.8200667\n",
      "epoch 499 : loss 0.7057089 ; accuracy 0.8203167\n",
      "test loss 0.6839852 ; accuracy 0.8252\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(500):\n",
    "    loss, accuracy = train_one_step(model, optimizer, \n",
    "                                    tf.constant(train_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "loss, accuracy = test(model, \n",
    "                      tf.constant(test_data[0], dtype=tf.float32), \n",
    "                      tf.constant(test_data[1], dtype=tf.int64))\n",
    "\n",
    "print('test loss', loss.numpy(), '; accuracy', accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
